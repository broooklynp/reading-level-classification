{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c4db66",
   "metadata": {},
   "source": [
    "# Educational Reading Level Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38566e6c",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d048a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, evaluate, accelerate\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620b296",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cf9a800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fiction and nonfiction data\n",
    "all_fiction_data = pd.read_csv(\"data/fiction.csv\")\n",
    "all_nonfiction_data = pd.read_csv(\"data/nonfiction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d748148a-7f5c-4144-aacb-6087c27c62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data randomly\n",
    "all_fiction_data = all_fiction_data.sample(frac=1)\n",
    "all_nonfiction_data = all_nonfiction_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ba4f7-4a68-4a9a-ab28-e89dd348e616",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a2f30294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for consistency\n",
    "all_fiction_data = all_fiction_data.rename(columns={\"passage\": \"text\",\n",
    "                                                    \"reading_level\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "81183887-e734-4000-a54a-5477df58ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['High', 'Middle ', 'High ', 'Elementary', 'Middle'], dtype=object)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique classes\n",
    "all_fiction_data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dc46a9fa-1f18-49d7-af23-794a7e913907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine classes that are the same\n",
    "map_levels = {\n",
    "    'Middle ': 'Middle',\n",
    "    'High ': 'High'\n",
    "}\n",
    "\n",
    "all_fiction_data['label'] = all_fiction_data['label'].replace(map_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f2618847-88ec-45c5-8b44-3af65a553750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Middle        1746\n",
       "High          1741\n",
       "Elementary    1661\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check amount of data in each class\n",
    "all_fiction_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ba9d79d5-e9de-4837-8fd0-7d5801b64bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>When she wrote about that night, she held no a...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4899</th>\n",
       "      <td>I talked to her almost every day of working ha...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4910</th>\n",
       "      <td>However, though he took these freedoms with me...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154</th>\n",
       "      <td>She really does care about the Doctor but she ...</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>The chauffeur, who had been opening something,...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>\"No, but he wishes he'd never been born. Mothe...</td>\n",
       "      <td>Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>She led him round the laurel path and to the w...</td>\n",
       "      <td>Elementary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>Even if they took him, she said, she would go ...</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>Esperanza smiled. When the grapes delivered th...</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>He returned to America in 1912 because of one ...</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5148 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       label\n",
       "3692  When she wrote about that night, she held no a...        High\n",
       "4899  I talked to her almost every day of working ha...        High\n",
       "4910  However, though he took these freedoms with me...        High\n",
       "3154  She really does care about the Doctor but she ...      Middle\n",
       "4413  The chauffeur, who had been opening something,...        High\n",
       "...                                                 ...         ...\n",
       "413   \"No, but he wishes he'd never been born. Mothe...  Elementary\n",
       "402   She led him round the laurel path and to the w...  Elementary\n",
       "4428  Even if they took him, she said, she would go ...        High\n",
       "2084  Esperanza smiled. When the grapes delivered th...      Middle\n",
       "3057  He returned to America in 1912 because of one ...      Middle\n",
       "\n",
       "[5148 rows x 2 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data\n",
    "all_fiction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7e1a156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "all_fiction_data = Dataset.from_pandas(all_fiction_data)\n",
    "all_nonfiction_data = Dataset.from_pandas(all_nonfiction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250b310-f90f-4e41-ae6a-157c7710c0d1",
   "metadata": {},
   "source": [
    "## Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2b689983",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Elementary\", 1: \"Middle\", 2: \"High\"}\n",
    "label2id = {\"Elementary\": 0, \"Middle\": 1, \"High\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "efc90f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', '__index_level_0__'],\n",
       "    num_rows: 5148\n",
       "})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fiction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "566aac58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e3ccab17cb4bde96ed9bdc40284ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Turn labels into 0, 1, or 2\n",
    "def map_labels_to_number(example):\n",
    "  example[\"label\"] = label2id[example[\"label\"]]\n",
    "  return example\n",
    "\n",
    "fiction_data = all_fiction_data.map(map_labels_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d9b0792d-d1fa-4b88-8e02-2010b6d86221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "fiction_data = fiction_data.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0df4f530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 4118\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 1030\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiction_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6a26f",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3391f09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
    "                                          use_fast=True)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5fee3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(examples):\n",
    "    \"\"\"\n",
    "    Tokenize given example text and return the tokenized text.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     padding=True,\n",
    "                     truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "86597d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dc7333775f41dab2bd4b3edd24431f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d0d91660324277a24d45d0b8300ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4118\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1030\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map tokenize_text function to the dataset\n",
    "tokenized_dataset = fiction_data.map(function=tokenize_text,\n",
    "                              batched=True,\n",
    "                              batch_size=1000)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0ee3a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Key: text\n",
      "Train sample: Dorset greeted the sally with delight. “Oh, abominably—you’ve just hit it—keeps me awake at night. The doctors tell me that’s what has knocked my digestion out—being so infernally jealous of her.—I can’t eat a mouthful of this stuff, you know,” he added suddenly, pushing back his plate with a clouded countenance; and Lily, unfailingly adaptable, accorded her radiant attention to his prolonged denunciation of other people’s cooks, with a supplementary tirade on the toxic qualities of melted butter.\n",
      "Test sample: I never saw so much expression in an inanimate thing before, and we all know how much expression they have! I used to lie awake as a child and get more entertainment and terror out of blank walls and plain furniture than most children could find in a toy-store. I remember what a kindly wink the knobs of our big, old bureau used to have, and there was one chair that always seemed like a strong friend.\n",
      "\n",
      "[INFO] Key: label\n",
      "Train sample: 1\n",
      "Test sample: 1\n",
      "\n",
      "[INFO] Key: __index_level_0__\n",
      "Train sample: 2773\n",
      "Test sample: 2521\n",
      "\n",
      "[INFO] Key: input_ids\n",
      "Train sample: [101, 15367, 11188, 1996, 8836, 2007, 12208, 1012, 1523, 2821, 1010, 11113, 20936, 2532, 6321, 1517, 2017, 1521, 2310, 2074, 2718, 2009, 1517, 7906, 2033, 8300, 2012, 2305, 1012, 1996, 7435, 2425, 2033, 2008, 1521, 1055, 2054, 2038, 6573, 2026, 17886, 3258, 2041, 1517, 2108, 2061, 1999, 7512, 26827, 9981, 1997, 2014, 1012, 1517, 1045, 2064, 1521, 1056, 4521, 1037, 28462, 1997, 2023, 4933, 1010, 2017, 2113, 1010, 1524, 2002, 2794, 3402, 1010, 6183, 2067, 2010, 5127, 2007, 1037, 26761, 4175, 8189, 5897, 1025, 1998, 7094, 1010, 4895, 7011, 16281, 2135, 15581, 3085, 1010, 15802, 2098, 2014, 23751, 3086, 2000, 2010, 15330, 7939, 24101, 1997, 2060, 2111, 1521, 1055, 26929, 1010, 2007, 1037, 26215, 14841, 13662, 2006, 1996, 11704, 11647, 1997, 12501, 12136, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Test sample: [101, 1045, 2196, 2387, 2061, 2172, 3670, 1999, 2019, 27118, 3490, 8585, 2518, 2077, 1010, 1998, 2057, 2035, 2113, 2129, 2172, 3670, 2027, 2031, 999, 1045, 2109, 2000, 4682, 8300, 2004, 1037, 2775, 1998, 2131, 2062, 4024, 1998, 7404, 2041, 1997, 8744, 3681, 1998, 5810, 7390, 2084, 2087, 2336, 2071, 2424, 1999, 1037, 9121, 1011, 3573, 1012, 1045, 3342, 2054, 1037, 19045, 16837, 1996, 16859, 2015, 1997, 2256, 2502, 1010, 2214, 4879, 2109, 2000, 2031, 1010, 1998, 2045, 2001, 2028, 3242, 2008, 2467, 2790, 2066, 1037, 2844, 2767, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "[INFO] Key: attention_mask\n",
      "Train sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Test sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get two samples from the tokenized dataset\n",
    "train_tokenized_sample = tokenized_dataset[\"train\"][0]\n",
    "test_tokenized_sample = tokenized_dataset[\"test\"][0]\n",
    "\n",
    "for key in train_tokenized_sample.keys():\n",
    "    print(f\"[INFO] Key: {key}\")\n",
    "    print(f\"Train sample: {train_tokenized_sample[key]}\")\n",
    "    print(f\"Test sample: {test_tokenized_sample[key]}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d3ccd",
   "metadata": {},
   "source": [
    "## Set Up Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "51bbe656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d50b58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n",
    "  \"\"\"\n",
    "  Computes the accuracy of a model by comparing the predictions and labels.\n",
    "  \"\"\"\n",
    "  predictions, labels = predictions_and_labels\n",
    "\n",
    "  # Get highest prediction probability of each prediction if predictions are probabilities\n",
    "  if len(predictions.shape) >= 2:\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "  return accuracy_metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e66fa7",
   "metadata": {},
   "source": [
    "## Set Up Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b96959fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Setup model for fine-tuning with classification head (top layers of network)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9c4271ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainable_parameters': 66955779, 'total_parameters': 66955779}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_params(model):\n",
    "    \"\"\"\n",
    "    Count the parameters of a PyTorch model.\n",
    "    \"\"\"\n",
    "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n",
    "\n",
    "# Count # parameters of the model\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac218bf9",
   "metadata": {},
   "source": [
    "All parameters in the model are trainable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc65689",
   "metadata": {},
   "source": [
    "### Create Directory for Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4694bff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('models/reading_level_text_classifier-distilbert-base-uncased')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model output directory\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create model save name\n",
    "model_save_name = \"reading_level_text_classifier-distilbert-base-uncased\"\n",
    "\n",
    "# Create model save path\n",
    "model_save_dir = Path(models_dir, model_save_name)\n",
    "\n",
    "model_save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee0e83",
   "metadata": {},
   "source": [
    "### Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9d87270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model checkpoints to: models/reading_level_text_classifier-distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "print(f\"[INFO] Saving model checkpoints to: {model_save_dir}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_save_dir,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_checkpointing=False,\n",
    "    no_cuda=True,\n",
    "    save_total_limit=3, # limit the total amount of save checkpoints (so we don't save num_epochs checkpoints)\n",
    "    use_cpu=False, # set to False by default, will use CUDA GPU or MPS device if available\n",
    "    seed=42, # set to 42 by default for reproducibility\n",
    "    load_best_model_at_end=True, # load the best model when finished training\n",
    "    logging_strategy=\"epoch\", # log training results every epoch\n",
    "    report_to=\"none\", # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n",
    "    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n",
    "    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n",
    "    hub_private_repo=False # optional: make the uploaded model private (defaults to False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48539c2c",
   "metadata": {},
   "source": [
    "### Set Up Trainer Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4a59c58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4118\n",
       "})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "378cfa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/czk9c7gs2fdc_1vc3t8gf_2h0000gn/T/ipykernel_11855/3169415646.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Setup Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9a6c5",
   "metadata": {},
   "source": [
    "## Training Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db794e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='1545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  13/1545 01:58 < 4:34:58, 0.09 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a text classification model\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac968a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330b1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8adb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
